{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dylan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "**Ideas:**  \n",
    "Split data into blocks, maybe store the data 3 dimensionally.  \n",
    "\n",
    "**Dimensions:**  \n",
    "- Location\n",
    "- Block (10 sequential days)\n",
    "- Day\n",
    "- Weather data\n",
    "  \n",
    "**Test Train Split Legend:**  \n",
    "Source  \n",
    "[[4, 5, 1, 9],  \n",
    " [2, 9, 5, 6],  \n",
    " [9, 5, 1, 8],  \n",
    " [8, 1, 2, 7]]  \n",
    "Features (X) → Target (Y)  \n",
    "[[4, 5, 1]   → [[9]  \n",
    " [2, 9, 5]   →  [6]  \n",
    " [9, 5, 1]   →  [8]  \n",
    " [8, 1, 2]   →  [7]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from datetime import datetime, date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./weatherAUS.csv')\n",
    "# Remove columns that either has a large amount of missing data or are not suitable for machine learning\n",
    "raw_data.drop(columns=['Sunshine', 'Evaporation', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow'], inplace=True)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_data.isnull().sum())\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing data\n",
    "data_filled = raw_data.copy()\n",
    "data_filled.fillna({'MinTemp': data_filled['MinTemp'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'MaxTemp': data_filled['MaxTemp'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Temp9am': data_filled['Temp9am'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Temp3pm': data_filled['Temp3pm'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Rainfall': data_filled['Rainfall'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'WindGustSpeed': data_filled['WindGustSpeed'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'WindSpeed9am': data_filled['WindSpeed9am'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'WindSpeed3pm': data_filled['WindSpeed3pm'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Humidity9am': data_filled['Humidity9am'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Humidity3pm': data_filled['Humidity3pm'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Pressure9am': data_filled['Pressure9am'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Pressure3pm': data_filled['Pressure3pm'].interpolate()}, inplace=True)\n",
    "data_filled.fillna({'Cloud9am': 0}, inplace=True)\n",
    "data_filled.fillna({'Cloud3pm': 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_filled.isnull().sum())\n",
    "data_filled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_data = data_filled.copy()\n",
    "fig, axs = plt.subplots(2, 4, figsize=(22, 20), sharey=False)\n",
    "\n",
    "# MinTemp MaxTemp Temp9am Temp3pm\n",
    "axs[0][0].set_title('Temperature')\n",
    "temperature_labels = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm']\n",
    "temperatures = visualisation_data[temperature_labels]\n",
    "axs[0][0].boxplot(temperatures, tick_labels=temperature_labels)\n",
    "\n",
    "# Rainfall Evaporation\n",
    "axs[0][1].set_title('Rainfall')\n",
    "rainfall = visualisation_data['Rainfall']\n",
    "axs[0][1].boxplot(rainfall)\n",
    "\n",
    "# WindGustSpeed WindSpeed9am WindSpeed3pm\n",
    "axs[0][2].set_title('Wind speed')\n",
    "wind_speed_labels = ['WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm']\n",
    "wind_speed = visualisation_data[wind_speed_labels]\n",
    "axs[0][2].boxplot(wind_speed, tick_labels=wind_speed_labels)\n",
    "\n",
    "# Humidity9am Humidity3pm\n",
    "axs[0][3].set_title('Humidity')\n",
    "humidity_labels = ['Humidity9am', 'Humidity3pm']\n",
    "humidity = visualisation_data[humidity_labels]\n",
    "axs[0][3].boxplot(humidity, tick_labels=humidity_labels)\n",
    "\n",
    "# Pressure9am Pressure3pm\n",
    "axs[1][0].set_title('Pressure')\n",
    "pressure_labels = ['Pressure9am', 'Pressure3pm']\n",
    "pressure = visualisation_data[pressure_labels]\n",
    "axs[1][0].boxplot(pressure, tick_labels=pressure_labels)\n",
    "\n",
    "# Cloud9am Cloud3pm\n",
    "axs[1][1].set_title('Cloud')\n",
    "cloud_labels = ['Cloud9am', 'Cloud3pm']\n",
    "cloud = visualisation_data[cloud_labels]\n",
    "axs[1][1].boxplot(cloud, tick_labels=cloud_labels)\n",
    "\n",
    "## Year Location\n",
    "#axs[1][2].set_title('Year range per location')\n",
    "#axs[1][2].scatter(visualisation_data['Year'], visualisation_data['Location'])\n",
    "\n",
    "## Month Day\n",
    "#axs[1][3].set_title('Total day range per location')\n",
    "#date = visualisation_data['Year'] * 353 + (visualisation_data['Month'] - 1) * 32 + visualisation_data['Day']\n",
    "#axs[1][3].scatter(date, visualisation_data['Location'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = data_filled.drop(columns=['Location', 'Date']).corr()\n",
    "sb.heatmap(corr_matrix, annot=True, cmap='viridis', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_iso_date(_date: str) -> str:\n",
    "\t'''Converts the date from dd-mm-yyyy to yyyy-mm-dd'''\n",
    "\treturn datetime.strptime(_date, '%d-%m-%Y').strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def convert_date_to_day_index(_date: str):\n",
    "\t'''Converts the date into the number of days since 2000-01-01'''\n",
    "\tdelta = datetime.strptime(_date, '%Y-%m-%d').date() - date(2000, 1, 1)\n",
    "\treturn delta.days\n",
    "\n",
    "def extract_year(_date: str) -> int:\n",
    "\t'''Returns the year component of the date'''\n",
    "\treturn datetime.strptime(_date, '%Y-%m-%d').year\n",
    "\n",
    "def extract_month(_date: str) -> int:\n",
    "\t'''Returns the month component of the date'''\n",
    "\treturn datetime.strptime(_date, '%Y-%m-%d').month\n",
    "\n",
    "def extract_day(_date: str) -> int:\n",
    "\t'''Returns the day component of the date'''\n",
    "\treturn datetime.strptime(_date, '%Y-%m-%d').day\n",
    "\n",
    "redated = data_filled.copy()\n",
    "redated['Date'] = redated['Date'].apply(to_iso_date)\n",
    "redated['DayIndex'] = redated['Date'].apply(convert_date_to_day_index)\n",
    "# TODO May need to remove year to prevent overfitting\n",
    "redated['Year'] = redated['Date'].apply(extract_year)\n",
    "redated['Month'] = redated['Date'].apply(extract_month)\n",
    "# Don't include the day to make it harder for the model to overfit\n",
    "#redated['Day'] = redated['Date'].apply(extract_day)\n",
    "# Remove the date as well for the same reason\n",
    "redated.drop(columns=['Date'], inplace=True)\n",
    "redated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_location(_location: str) -> int:\n",
    "\t'''Converts the string into bytes then reencodes it to an int.'''\n",
    "\treturn int.from_bytes(_location.encode(), 'big')\n",
    "\n",
    "hashed = redated.copy()\n",
    "hashed['LocationHash'] = hashed['Location'].apply(hash_location)\n",
    "hashed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconfigure dataframe\n",
    "https://pandas.pydata.org/docs/user_guide/advanced.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconfigure(_df: pd.DataFrame, _block_size=5):\n",
    "\t'''Splits the rows into blocks up to a max size defined by block_size. Blocks are per location. Uses Location, Date, Block, and Id as the labels for a multiIndex DataFrame.'''\n",
    "\t# Lists to store block and id number\n",
    "\tblock = []\n",
    "\tid = []\n",
    "\n",
    "\t# Goes through every location\n",
    "\tfor _, group in _df.groupby('Location', sort=False):\n",
    "\t\tblock_num = 0\n",
    "\t\tid_num = 0\n",
    "\t\tprev = group['DayIndex'].iloc[0]\n",
    "\n",
    "\t\t# Iterate over the DayIndex column\n",
    "\t\tfor _, index_num in group['DayIndex'].items():\n",
    "\t\t\t# Check if a new block should be started\n",
    "\t\t\tif id_num == _block_size or (index_num != prev + 1):\n",
    "\t\t\t\tblock_num += 1\n",
    "\t\t\t\tid_num = 0  # Reset ID within the block\n",
    "\n",
    "\t\t\t# Append the block number and ID within block to the lists\n",
    "\t\t\tblock.append(block_num)\n",
    "\t\t\tid.append(id_num)\n",
    "\n",
    "\t\t\t# Update variables for the next iteration\n",
    "\t\t\tid_num += 1\n",
    "\t\t\tprev = index_num\n",
    "\n",
    "\t# Create the multiIndex\n",
    "\tindex = pd.MultiIndex.from_arrays(\n",
    "\t\t[_df['Location'], block, id],\n",
    "\t\tnames=['Location', 'Block', 'Id']\n",
    "\t)\n",
    "\n",
    "\t# Removed unneeded columns and apply the multiIndex\n",
    "\tstripped = _df.drop(columns=['Location'])\n",
    "\tstripped.set_index(index, inplace=True)\n",
    "\treturn stripped\n",
    "\n",
    "reconfigured = reconfigure(hashed, block_size)\n",
    "reconfigured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge(_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\t'''Purge blocks with less than 10 elements in them.'''\n",
    "\t# Count rows in each block\n",
    "\tblock_sizes = _df.groupby(['Location', 'Block'], sort=False).size()\n",
    "\n",
    "\t# Identify unfit blocks (those with less than block_size)\n",
    "\t# Pylance mistakes this for an error\n",
    "\tunfit = block_sizes[block_sizes < block_size].index.to_list()\n",
    "\n",
    "\t# Boolean indexing to drop multiple combinations\n",
    "\tdf_filtered = _df[~((_df.index.get_level_values('Location').isin([x[0] for x in unfit])) &\n",
    "                        _df.index.get_level_values('Block').isin([x[1] for x in unfit]))]\n",
    "\n",
    "\treturn df_filtered\n",
    "\n",
    "purged = purge(reconfigured)\n",
    "purged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_group(_group: pd.DataFrame):\n",
    "\tfeatures = _group.iloc[:-1]  # First 9 rows as features\n",
    "\ttarget = _group.iloc[-1]     # 10th row as the target\n",
    "\t# ERROR: inconsistent array lengths\n",
    "\treturn features, target\n",
    "\n",
    "def gpt_split_into_features_and_target(_df: pd.DataFrame):\n",
    "\t'''Split a dataframe with groups into features and targets lists.'''\n",
    "\tfeatures_list = []\n",
    "\ttargets_list = []\n",
    "\tbroken_counter = 0\n",
    "\n",
    "\t# The issue is that this doesn't result in blocks of 10\n",
    "\tfor _, group in _df.groupby(['Location', 'Block'], sort=False):\n",
    "\t\tif len(group) != 10:\n",
    "\t\t\tbroken_counter += 1\n",
    "\t\tfeatures, target = divide_group(group)\n",
    "\t\tfeatures_list.append(features.values.flatten())  # Flatten the features into one row\n",
    "\t\ttargets_list.append(target.values[0])  # Single target value\n",
    "\n",
    "\tprint(broken_counter)\n",
    "\n",
    "\t# Convert lists to arrays for scikit-learn\n",
    "\tfeatures = np.array(features_list)\n",
    "\ttargets = np.array(targets_list)\n",
    "\treturn features, targets\n",
    "\n",
    "X, Y = gpt_split_into_features_and_target(purged)\n",
    "\n",
    "print(f'X has {X.shape[0]} samples, Y has {Y.shape[0]} samples.')\n",
    "\n",
    "#X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "## Note: 0.25 x 0.8 = 0.2, so the validation set is 20% of the original dataset\n",
    "#X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.25, random_state=42)\n",
    "#print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "#print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "#print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "#locations = multi_indexed.index.get_level_values('Location').drop_duplicates().to_list()\n",
    "#albury = multi_indexed.xs('Albury')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regresssion_model = LinearRegression()\n",
    "linear_regresssion_model.fit(X_train, Y_train)\n",
    "\n",
    "ridge_regression_model = Ridge()\n",
    "ridge_regression_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_y_pred = linear_regresssion_model.predict(X_test)\n",
    "ridge_y_pred = ridge_regression_model.predict(X_test)\n",
    "\n",
    "print('Linear Regression Evaluation:')\n",
    "print(f'Mean Squared Error: {mean_squared_error(Y_test, linear_y_pred):.2f}')\n",
    "print(f'R^2 Score: {r2_score(Y_test, linear_y_pred):.2f}')\n",
    "\n",
    "print('\\nRidge Regression Evaluation:')\n",
    "print(f'Mean Squared Error: {mean_squared_error(Y_test, ridge_y_pred):.2f}')\n",
    "print(f'R^2 Score: {r2_score(Y_test, ridge_y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately a failure, but kept for posterity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_model = KMeans(n_clusters = 6)\n",
    "k_means_model.fit(X=purged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pred = k_means_model.predict(purged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the different column names for use in the visualisation below\n",
    "purged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot kmeans results\n",
    "axes = ['MaxTemp', 'Rainfall']\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(purged[axes[0]], purged[axes[1]], c=cluster_pred, cmap='viridis')\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel(axes[0])\n",
    "plt.ylabel(axes[1])\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
